{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于深度学习的犬类识别研究与实现（源代码）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 导入程序所需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import shutil\n",
    "import seaborn \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "# 设置 (CPU) 生成随机数的种子，一旦固定种子，后面依次生成的随机数都是固定的\n",
    "# torch.manual_seed(42)\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.models import resnet\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
    "from d2l import torch as d2l\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] # 正常显示中文\n",
    "plt.rcParams['axes.unicode_minus'] = False   # 正常显示负号\n",
    "seaborn.set(font='SimHei')  # 解决Seaborn中文显示问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看设备信息和语言版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PyTorch:', torch.__version__)\n",
    "print('设备:', torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU')\n",
    "print('Python: ', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始数据集在old_path目录里面\n",
    "old_path = '../input/stanford-dogs-dataset/images/Images'\n",
    "\n",
    "# 划分好的数据集放在data_path目录里\n",
    "# 本机: \n",
    "# data_path = '../input/stanford-dogs-dataset/images/train_val_test'\n",
    "# kaggle: ../input/stanford-dogs-dataset-splited-pl/train_val_test\n",
    "data_path = '../input/stanford-dogs-dataset-splited-pl/train_val_test'\n",
    "\n",
    "# result_path = './working'\n",
    "# kaggle  ./\n",
    "result_path = './'\n",
    "\n",
    "# presplit为True表示数据集已划分，为False表示还未划分\n",
    "presplit = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找训练集、测试集和验证集，若有则将presplit置为True\n",
    "if os.path.exists(os.path.join(data_path, 'training_set')) & \\\n",
    "   os.path.exists(os.path.join(data_path, 'test_set'))     & \\\n",
    "   os.path.exists(os.path.join(data_path, 'val_set')) :\n",
    "    presplit = True\n",
    "    \n",
    "else :\n",
    "    presplit = False\n",
    "    \n",
    "print('presplit: ', presplit)\n",
    "if presplit :\n",
    "    print('数据已划分为训练集、测试集和验证集') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练集、验证集和测试集\n",
    "\n",
    "1. 统计每一个类别文件夹class_folder(labels)下的文件数量（图像数量image_amount）\n",
    "\n",
    "2. 根据图像数量生成随机整数作为图像索引，再用random_split划分训练集、验证集和测试集索引\n",
    "\n",
    "3. 根据索引将对应图像复制到目的文件夹下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制文件\n",
    "def copyfile(old_dir, new_dir, filename):\n",
    "    \"\"\"将文件复制到目标目录\"\"\"\n",
    "    if os.path.exists(os.path.join(new_dir, filename)) :\n",
    "        # 若该文件名已存在，则先将其删除，再进行复制\n",
    "        os.remove(os.path.join(new_dir, filename))\n",
    "        shutil.copy(old_dir, os.path.join(new_dir, filename))\n",
    "    else :\n",
    "        # 若该文件名以上的文件夹不存在，则先创建文件夹，再进行复制\n",
    "        os.makedirs(new_dir, exist_ok = True)\n",
    "        shutil.copy(old_dir, os.path.join(new_dir, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_train_val_test(old_path, new_path, class_folders, training_ratio, val_ratio, test_ratio):\n",
    "    # {\"class_folder_name\" : [image_name, ...]}\n",
    "    class_dict = {}\n",
    "    # {\"class_name\" : image_ammount}\n",
    "    image_amount_dict = {}\n",
    "    # 类别总数\n",
    "    class_amount = len(class_folders)\n",
    "    \n",
    "    for i in range(class_amount) :\n",
    "        print(f'对{class_folders[i][10 : ]}进行划分')\n",
    "        # 该类别所在的文件夹\n",
    "        class_images = os.listdir(old_path + '/' + class_folders[i])\n",
    "        # 该类别文件夹下的图像数量\n",
    "        image_amount_dict[class_names[i]] = len(class_images)\n",
    "        image_amount = image_amount_dict[class_names[i]]\n",
    "        # print(class_folders[i] + ':' + str(image_amount))\n",
    "        # 该类别文件夹下的所有图像名称\n",
    "        class_dict[class_folders[i]] = os.listdir(old_path + '/' + class_folders[i])\n",
    "        \n",
    "        # 生成随机整数作为图像索引\n",
    "        index_list = [index for index in range(0, image_amount)]\n",
    "        training_size = int(training_ratio * image_amount)\n",
    "        val_size    = int(val_ratio    * image_amount)\n",
    "        test_size     = image_amount - training_size - val_size\n",
    "        # 将索引列表随机分为训练集、验证集和测试集\n",
    "        training_index_list, val_index_list, test_index_list = \\\n",
    "                torch.utils.data.random_split(index_list, [training_size, val_size, test_size])\n",
    "        training_index_list = list(training_index_list)\n",
    "        val_index_list    = list(val_index_list) \n",
    "        test_index_list     = list(test_index_list)\n",
    "\n",
    "        # 划分训练集\n",
    "        for train_index in training_index_list :\n",
    "            old_dir = os.path.join(old_path, class_folders[i], class_dict[class_folders[i]][train_index])\n",
    "            new_dir = os.path.join(new_path, 'training_set', class_folders[i])\n",
    "            filename = class_dict[class_folders[i]][train_index]\n",
    "            copyfile(old_dir , new_dir, filename)\n",
    "        # 划分验证集\n",
    "        for val_index in val_index_list :\n",
    "            old_dir = os.path.join(old_path, class_folders[i], class_dict[class_folders[i]][val_index])\n",
    "            new_dir = os.path.join(new_path, 'val_set', class_folders[i])\n",
    "            filename = class_dict[class_folders[i]][val_index]\n",
    "            copyfile(old_dir , new_dir, filename)\n",
    "        # 划分测试集\n",
    "        for test_index in test_index_list :\n",
    "            old_dir = os.path.join(old_path, class_folders[i], class_dict[class_folders[i]][test_index])\n",
    "            new_dir = os.path.join(new_path, 'test_set', class_folders[i])\n",
    "            filename = class_dict[class_folders[i]][test_index]\n",
    "            copyfile(old_dir , new_dir, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "划分训练集（0.5）、验证集（0.25）和测试集（0.25）\n",
    "\"\"\"\n",
    "if presplit :\n",
    "    # 数据已划分为训练集、测试集和验证集，不用再划分\n",
    "    print('数据已划分为训练集、测试集和验证集') \n",
    "    \n",
    "else :\n",
    "    # 数据还未划分为训练集、测试集和验证集，将其进行随机划分\n",
    "    print('数据还未划分为训练集、测试集和验证集')\n",
    "    \n",
    "    training_ratio = 0.5\n",
    "    test_ratio     = 0.25\n",
    "    val_ratio    = 0.25\n",
    "\n",
    "    # 每个类别所在的文件夹名\n",
    "    class_folders = [i  for i in os.listdir(old_path)] \n",
    "    # 每个类别名称\n",
    "    # class_names = [name[10 : ] for name in class_folders]\n",
    "    # 总类别数\n",
    "    # class_amount = len(class_folders)\n",
    "    # 第0个类别的图像名称\n",
    "    # class_images = os.listdir(old_path + '/' + class_folders[0])\n",
    "\n",
    "    # print('class_folders: ', class_folders)      # class_folders: ['n02085620-Chihuahua', 'n02085782-Japanese_spaniel',...]\n",
    "    # print(class_folders[0][10 : ])               # Chihuahua\n",
    "    # print('class_names: ', class_names)  # class_names:  ['Chihuahua', 'Japanese_spaniel', ...]\n",
    "    # print('class_amount: ', class_amount)   # class_amount:  120\n",
    "    # print(class_images)      # ['n02085620_10074.jpg', 'n02085620_10131.jpg', ...]\n",
    "    \n",
    "    reorg_train_val_test(old_path, data_path, class_folders, training_ratio, val_ratio, test_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画出数据分布图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条形图（竖状）数量标识\n",
    "def count_text_v(ax, item_dict): \n",
    "    i = 0\n",
    "    for p in ax.patches:\n",
    "        # val = p.get_height()                    # 条形的高度\n",
    "        x   = p.get_x() + p.get_width() / 2   # x坐标\n",
    "        y   = p.get_y() + p.get_height()         # y坐标\n",
    "        plt.text(x = x, y = y, s = str(list(item_dict.values())[i]), ha = 'center', va = 'bottom',\n",
    "             fontdict = dict(fontsize = 20, color = 'black'))\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def plot_vertical_images_per_class(data_path, mode, title, save_name, row_size, col_size, y, color):\n",
    "    data_folder = data_path + '/' + mode + '/'\n",
    "    item_dict   = {(root.split('/')[-1])[10 : ] : len(files) for root, _, files in os.walk(data_folder)}\n",
    "    tmp_list    = sorted(item_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "    item_dict.clear()\n",
    "    for rank, (key, value) in enumerate(tmp_list, 1):\n",
    "        item_dict[key] = value\n",
    "    item_dict.pop('')\n",
    "   \n",
    "    plt.figure(figsize = (row_size, col_size))\n",
    "    ax = plt.bar(list(item_dict.keys())[0 : ], list(item_dict.values())[0 : ], width = 0.6, color = color)\n",
    "    count_text_v(ax, item_dict)\n",
    "    # 添加水平直线，y = 0表示垂线过y = 0\n",
    "    plt.axhline(y = min(item_dict.values()), ls= \"--\" , color = \"#f4320c\", linewidth = 5)\n",
    "    plt.text(y = min(item_dict.values()), x = -1.3, s = str(min(item_dict.values())), ha = 'right', va = 'center',\n",
    "             fontdict = dict(fontsize = 30, color = 'black'))\n",
    "    plt.title(title, fontsize = 50)\n",
    "    plt.xticks(ticks = range(0, 120, 1), labels = list(item_dict.keys())[0 : ], rotation = 90, fontsize = 25)\n",
    "    plt.yticks(ticks = y, fontsize = 30)\n",
    "    #设置x轴的范围\n",
    "    plt.xlim( xmin = -1, xmax = 120)\n",
    "    #设置y轴的范围\n",
    "    plt.ylim(ymin = 0, ymax = y[-1])\n",
    "    plt.xlabel('Classes', fontsize = 40)\n",
    "    plt.ylabel('Number of Images', fontsize = 40)\n",
    "    # plt.show()\n",
    "    # plt.savefig(\"./minist.jpg\")\n",
    "    plt.savefig(\"./working/\" + save_name + \".jpg\", bbox_inches = 'tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条形图（横状）数量标识\n",
    "def count_text_h(ax, item_dict): \n",
    "    i = 0\n",
    "    for p in ax.patches:\n",
    "        # val = p.get_width()                    # 条形的长度\n",
    "        x   = p.get_x() + p.get_width()        # x坐标\n",
    "        y   = p.get_y() + p.get_height() / 2   # y坐标\n",
    "        plt.text(x = x, y = y, s = str(list(item_dict.values())[i]), ha = 'left', va = 'center',\n",
    "             fontdict = dict(fontsize = 38, color = 'black'))\n",
    "        i += 1\n",
    "\n",
    "def plot_honrizontal_images_per_class(data_path, mode, title, save_name, row_size, col_size, y, color):\n",
    "    data_folder = data_path + '/' + mode + '/'\n",
    "    item_dict   = {(root.split('/')[-1])[10 : ] : len(files) for root, _, files in os.walk(data_folder)}\n",
    "    tmp_list    = sorted(item_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "    item_dict.clear()\n",
    "    for rank, (key, value) in enumerate(tmp_list, 1):\n",
    "        item_dict[key] = value\n",
    "    item_dict.pop('')\n",
    "   \n",
    "    plt.figure(figsize = (row_size, col_size))\n",
    "    ax = plt.barh(list(item_dict.keys())[0 : ], list(item_dict.values())[0 : ], height = 0.8, color = color)\n",
    "    count_text_h(ax, item_dict)\n",
    "    \n",
    "    # 添加垂直直线，x = 0表示垂线过x = 0，其余参数含义同上\n",
    "    plt.axvline(x = min(item_dict.values()), ls= \"--\" , color = \"#f4320c\", linewidth = 5)\n",
    "    plt.text(x = min(item_dict.values()), y = -2.07, s = str(min(item_dict.values())), ha = 'center', va = 'bottom',\n",
    "             fontdict = dict(fontsize = 38, color = 'black'))\n",
    "    plt.title(title, fontsize = 64)\n",
    "    plt.yticks(ticks = range(0, 120, 1), labels = list(item_dict.keys())[0 : ], fontsize = 40)\n",
    "    plt.xticks(ticks = y, fontsize = 38)\n",
    "    #设置x轴的范围\n",
    "    plt.xlim( xmin = 100, xmax = y[-1])\n",
    "    #设置y轴的范围\n",
    "    plt.ylim(ymin = -1, ymax = 120)\n",
    "    plt.ylabel('类别', fontsize = 60)\n",
    "    plt.xlabel('图像数量', fontsize = 60)\n",
    "    # plt.show()\n",
    "    # plt.savefig(\"./minist.jpg\")\n",
    "    plt.savefig(\"./working/\" + save_name + \".jpg\", bbox_inches = 'tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条形图\n",
    "\n",
    "y          = [i for i in range(0, 280, 20)]\n",
    "y_training = [i for i in range(0, 140, 10)]\n",
    "y_val    = [i for i in range(0, 80, 10)]\n",
    "y_test     = [i for i in range(0, 80, 10)]\n",
    "\n",
    "if presplit :\n",
    "    plot_honrizontal_images_per_class(old_path, mode = '', \n",
    "                          # title     = 'Distribution of Classes', \n",
    "                          title     = '斯坦福犬类数据集各类别数量分布', \n",
    "                          save_name = 'Distribution_h_of_Classes', \n",
    "                          row_size  = 26, col_size = 40, y = y, color = '#5684ae')\n",
    "\n",
    "    plot_honrizontal_images_per_class(os.path.join(data_path, 'training_set'), mode = '', \n",
    "                          # title     = 'Distribution of Training_Set Classes', \n",
    "                          title     = '训练集各类别数量分布', \n",
    "                          save_name = 'Distribution_h_of_Training_Set_Classes', \n",
    "                          row_size  = 26, col_size = 40, y = y_training, color = '#5684ae')\n",
    "\n",
    "    plot_honrizontal_images_per_class(os.path.join(data_path, 'val_set'), mode = '', \n",
    "                          # title     = 'Distribution of Val_Set Classes', \n",
    "                          title     = '验证集各类别数量分布', \n",
    "                          save_name = 'Distribution_h_of_Val_Set_Classes', \n",
    "                          row_size  = 26, col_size = 40, y = y_val, color = '#5684ae')\n",
    "\n",
    "    plot_honrizontal_images_per_class(os.path.join(data_path, 'test_set'), mode = '', \n",
    "                          # title     = 'Distribution of Test_Set Classes', \n",
    "                          title     = '测试集各类别数量分布', \n",
    "                          save_name = 'Distribution_h_of_Test_Set_Classes', \n",
    "                          row_size  = 26, col_size = 40, y = y_test, color = '#5684ae')\n",
    "    \n",
    "else :\n",
    "    plot_honrizontal_images_per_class(old_path, mode = '', \n",
    "                          title     = 'Distribution of Classes', \n",
    "                          save_name = 'Distribution_h_of_Classes', \n",
    "                          row_size  = 26, col_size = 40, y = y, color = '#5684ae')\n",
    "\n",
    "# off blue  #5684ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 柱状图\n",
    "if presplit :\n",
    "    plot_vertical_images_per_class(old_path, mode = '', \n",
    "                          title     = 'Distribution of Classes', \n",
    "                          save_name = 'Distribution_of_Classes', \n",
    "                          row_size  = 64, col_size = 42, y = y, color = sns.xkcd_rgb['bronze'])\n",
    "\n",
    "    plot_vertical_images_per_class(os.path.join(data_path, 'training_set'), mode = '', \n",
    "                          title     = 'Distribution of Training_Set Classes', \n",
    "                          save_name = 'Distribution_of_Training_Set_Classes', \n",
    "                          row_size  = 64, col_size = 42, y = y_training, color = sns.xkcd_rgb['bronze'])\n",
    "\n",
    "    plot_vertical_images_per_class(os.path.join(data_path, 'val_set'), mode = '', \n",
    "                          title     = 'Distribution of Val_Set Classes', \n",
    "                          save_name = 'Distribution_of_Val_Set_Classes', \n",
    "                          row_size  = 64, col_size = 42, y = y_val, color = sns.xkcd_rgb['bronze'])\n",
    "\n",
    "    plot_vertical_images_per_class(os.path.join(data_path, 'test_set'), mode = '', \n",
    "                          title     = 'Distribution of Test_Set Classes', \n",
    "                          save_name = 'Distribution_of_Test_Set_Classes', \n",
    "                          row_size  = 64, col_size = 42, y = y_test, color = sns.xkcd_rgb['bronze'])\n",
    "    \n",
    "    \n",
    "else :\n",
    "    plot_vertical_images_per_class(old_path, mode = '', \n",
    "                          title     = 'Distribution of Classes', \n",
    "                          save_name = 'Distribution_of_Classes', \n",
    "                          row_size  = 42, col_size = 42, y = y, color = sns.xkcd_rgb['bronze'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像增广\n",
    "\n",
    "#### 调用 torchvision.datasets.ImageFolder 返回训练数据与标签\n",
    "**1. torchvision.datasets.ImageFolder 有 root, transform, target_transform, loader四个参数，现在依次介绍这四个参数**\n",
    "\n",
    "**root**：图片存储的根目录，即各类别文件夹所在目录的上一级目录，在下面的例子中是’./data/train/’。\n",
    "\n",
    "**transform**：对图片进行预处理的操作（函数），原始图片作为输入，返回一个转换后的图片。\n",
    "\n",
    "**target_transform**：对图片类别进行预处理的操作，输入为 target，输出对其的转换。如果不传该参数，即对 target 不做任何转换，返回的顺序索引 0,1, 2…\n",
    "\n",
    "**loader**：表示数据集加载方式，通常默认加载方式即可。\n",
    "\n",
    "**2. 该 API 有以下成员变量:**\n",
    "\n",
    "**self.classes**：用一个 list 保存类别名称；\n",
    "\n",
    "**self.class_to_idx**：类别对应的索引，与不做任何转换返回的 target 对应；\n",
    "\n",
    "**self.imgs**：保存(img-path, class) tuple的 list；\n",
    "\n",
    "原文链接：https://blog.csdn.net/qq_33254870/article/details/103362621\n",
    "\n",
    "#### torch.utils.data.DataLoader\n",
    "\n",
    "https://blog.csdn.net/qq_36044523/article/details/118914223\n",
    "\n",
    "https://blog.csdn.net/qq_40788447/article/details/114937779\n",
    "\n",
    "#### 标准化（归一化）\n",
    "\n",
    "验证集的标准化和训练集一样\n",
    "\n",
    "测试集标准化的均值和标准差应该来源于训练集得到的均值和标准偏差\n",
    "\n",
    "在训练之前把数据划分成训练集和测试集，接着对训练集进行标准化，同时保存标准化时计算出来的参数，例如最大值最小值或者是方差之类的，具体由你使用的标准化方法而定。最后再用这些参数来标准化你的测试集，以及之后所有新的测试样本。\n",
    "\n",
    "由于该数据集源于ImageNet，因此可以直接用ImageNet的mean和std\n",
    "\n",
    "1) RandomRotation(RR)\n",
    "\n",
    "2) RandomResizedCrop(RRC)\n",
    "\n",
    "3) RandomHorizontalFlip(RHF)\n",
    "\n",
    "4) ColorJitter(CJ)\n",
    "\n",
    "5) RandomRotation + RandomHorizontalFlip(RR-RHF)\n",
    "\n",
    "6) RandomRotation + RandomResizedCrop + RandomHorizontalFlip(RR-RRC-RHF)\n",
    "\n",
    "6) RandomRotation+ColorJitter+RandomHorizontalFlip(RR-CJ-RHF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "图像转换为224 X 224\n",
    "'''\n",
    "def load_transform_images(images_path, batch_size, threads, mean, std):\n",
    "    # 对训练集进行图像增广\n",
    "    training_transform = transforms.Compose([\n",
    "                                        # 随机旋转, degree = 15, 20, 30, 25\n",
    "                                        transforms.RandomRotation(degrees = 20),\n",
    "                                        # 修改亮度brightness、对比度contrast、饱和度saturation和色调hue\n",
    "                                        transforms.ColorJitter(brightness = 0.4, contrast = 0.4, saturation = 0, hue = 0),\n",
    "                                        # 随机长宽比裁剪\n",
    "                                        # transforms.RandomResizedCrop((224, 224), scale=(0.7, 1.0), ratio=(0.85, 1.1)),\n",
    "                                        transforms.Resize((224, 224)),\n",
    "                                        # 水平翻转\n",
    "                                        transforms.RandomHorizontalFlip(p = 0.5),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        # test4进行数据标准化\n",
    "                                        transforms.Normalize(torch.Tensor(mean),\n",
    "                                                             torch.Tensor(std))\n",
    "                                        ])\n",
    "    # 验证集和测试集不进行图像增广\n",
    "    test_transform = transforms.Compose([\n",
    "                                        transforms.Resize((224,224)),\n",
    "                                        # transforms.CenterCrop((224,224)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(torch.Tensor(mean),\n",
    "                                                             torch.Tensor(std))\n",
    "                                        ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "                                        transforms.Resize((224,224)),\n",
    "                                        # transforms.CenterCrop((224,224)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(torch.Tensor(mean),\n",
    "                                                             torch.Tensor(std))\n",
    "                                        ])\n",
    "\n",
    "    training_set = datasets.ImageFolder(root = images_path + '/training_set', transform = training_transform)\n",
    "    val_set      = datasets.ImageFolder(root = images_path + '/val_set',      transform = val_transform)\n",
    "    test_set     = datasets.ImageFolder(root = images_path + '/test_set',     transform = test_transform)\n",
    "    dataset      = training_set\n",
    "    \n",
    "    #  数据集的加载器，自动将数据分割成mini-batch\n",
    "    training_set_loader = DataLoader(training_set, batch_size = batch_size, num_workers = threads, shuffle = True)\n",
    "    val_set_loader      = DataLoader(val_set,      batch_size = batch_size, num_workers = threads, shuffle = False)\n",
    "    test_set_loader     = DataLoader(test_set,     batch_size = batch_size, num_workers = threads, shuffle = False)\n",
    "\n",
    "    return training_set_loader, test_set_loader, val_set_loader, dataset, training_set, test_set, val_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean和std的取法\n",
    "为什么用mean = [0.485, 0.456, 0.406]，std = [0.229, 0.224, 0.225] ？\n",
    "\n",
    "因为这一组数据是从imagenet训练集中抽样算出来的。\n",
    "\n",
    "In that example, they are using the mean and stddev of ImageNet, but if you look at their MNIST examples, the mean and stddev are 1-dimensional (since the inputs are greyscale-- no RGB channels).\n",
    "\n",
    "Whether or not to use ImageNet's mean and stddev depends on your data. **Assuming your data are ordinary photos of \"natural scenes\"(people, buildings, animals, varied lighting/angles/backgrounds, etc.), and assuming your dataset is biased in the same way ImageNet is (in terms of class balance), then it's ok to normalize with ImageNet's scene statistics.** If the photos are \"special\" somehow (color filtered, contrast adjusted, uncommon lighting, etc.) or an \"un-natural subject\" (medical images, satellite imagery, hand drawings, etc.) then I would recommend correctly normalizing your dataset before model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size取128, 256\n",
    "batch_size   = 128\n",
    "threads      = 0\n",
    "# 进行数据标准化的均值和标准差\n",
    "mean         = [0.485, 0.456, 0.406]\n",
    "std          = [0.229, 0.224, 0.225]\n",
    "\n",
    "training_set_loader, test_set_loader, val_set_loader, dataset, training_set, test_set, val_set = \\\n",
    "                  load_transform_images(data_path, batch_size, threads, mean, std)\n",
    "\n",
    "class_names = dataset.classes\n",
    "class_names = [classes[10:] for classes in class_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示图像预处理效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"总共有\", len(class_names), \"种狗\")\n",
    "print(\"训练集大小：\", len(training_set))\n",
    "print(\"测试集大小：\", len(test_set))\n",
    "print(\"验证集大小：\", len(val_set))\n",
    "# print(class_names)\n",
    "# print(training_set[0].size())\n",
    "# print(training_set.shape)\n",
    "print(dataset[1])\n",
    "print(dataset.imgs[1])\n",
    "print(dataset.imgs[1][0])\n",
    "\n",
    "img = Image.open(dataset.imgs[1][0], \"r\")\n",
    "#img.show()\n",
    "plt.figure(\"sample dog\")\n",
    "plt.axis('off')  # 去掉坐标轴\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_images(training_set, batch_size, class_names, mean, std, rows, columns, size, title_size):\n",
    "    sampler = RandomSampler(training_set, num_samples = batch_size, replacement = True)\n",
    "    train_loader = DataLoader(training_set, sampler = sampler, shuffle = False, batch_size = batch_size, num_workers = 0)\n",
    "    \n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    plt.figure(figsize = (size, size))\n",
    "    \n",
    "    '''\n",
    "    # 九宫图\n",
    "    for i in range(rows * columns):\n",
    "        plt.subplot(rows, columns, i + 1)\n",
    "        plt.title(class_names[labels.numpy()[i]], fontsize = title_size)\n",
    "        img = images[i].permute(1, 2, 0)\n",
    "        # \n",
    "        # img = torch.tensor(std) * img + torch.tensor(mean)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img, interpolation = 'none')\n",
    "        # plt.savefig(\"./Sample_Images/sample_image\" + str(i) + \".jpg\")\n",
    "        # plt.tight_layout()\n",
    "        print(i + 1, ': ', class_names[labels.numpy()[i]])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_path, 'sample_image.png'), bbox_inches = 'tight')\n",
    "    '''\n",
    "        \n",
    "    # 单张图\n",
    "    for i in range(rows * columns):\n",
    "        # plt.subplot(rows, columns, i + 1)\n",
    "        plt.title(class_names[labels.numpy()[i]], fontsize = title_size * 2)\n",
    "        img = images[i].permute(1, 2, 0)\n",
    "        # img = torch.tensor(std) * img + torch.tensor(mean)\n",
    "        # img = images[i]\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img, interpolation = 'none')\n",
    "        plt.savefig(os.path.join(result_path, 'sample_image' + str(i) + '.png'), bbox_inches = 'tight')\n",
    "        # plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_images(training_set, batch_size, class_names, mean, std, rows = 3, columns = 3, size = 40, title_size = 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 获取网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(finetune_net, net_name, dropout_ratio, class_names, unfrozen_layers):\n",
    "    for name, child in finetune_net.named_children():\n",
    "        '''\n",
    "        print(name + ' is unfrozen')\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "        '''\n",
    "        \n",
    "        if name in unfrozen_layers:\n",
    "            print(name + ' is unfrozen')\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            print(name + ' is frozen')\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    # 对输出层重新设置\n",
    "    num_inftrs = finetune_net.fc.in_features\n",
    "    # 1. 输出层直接为一个全连接层\n",
    "    # finetune_net.fc = nn.Linear(num_inftrs, len(class_names))\n",
    "    # nn.init.kaiming_uniform_(tensor = finetune_net.fc.weight, a = 0, mode = 'fan_in')\n",
    "    \n",
    "    # 2. 输出层为一个Sequential子网络\n",
    "    finetune_net.fc = nn.Sequential(nn.Linear(num_inftrs, 256),\n",
    "                                    nn.ReLU(),\n",
    "                                    # test6,不使用dropout\n",
    "                                    nn.Dropout(p = dropout_ratio),\n",
    "                                    nn.Linear(256, len(class_names)))\n",
    "    # 对finetune_net的fc层权重进行何（恺明）初始化\n",
    "    # nn.init.kaiming_uniform_(tensor = finetune_net.fc.weight, a = 0, mode = 'fan_in')\n",
    "    nn.init.kaiming_uniform_(tensor = finetune_net.fc[0].weight, a = 0, mode = 'fan_in')\n",
    "    nn.init.kaiming_uniform_(tensor = finetune_net.fc[3].weight, a = 0, mode = 'fan_in')\n",
    "    \n",
    "    display(finetune_net)\n",
    "    \n",
    "    # torch.save(finetune_net.state_dict(), os.path.join(result_path, str(net_name) + '.pth'))\n",
    "    # finetune_net.load_state_dict(torch.load(os.path.join(result_path, str(net_name) + '.pth')))\n",
    "    \n",
    "    total_params = sum(param.numel() for param in finetune_net.parameters())\n",
    "    print(f'{total_params:,} total parameters')\n",
    "\n",
    "    total_trainable_params = sum(param.numel() for param in finetune_net.parameters() if param.requires_grad)\n",
    "    print(f'{total_trainable_params:,} training parameters')\n",
    "    \n",
    "    return finetune_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test6使用预训练模型进行微调\n",
    "finetune_net = resnet.resnet50(pretrained = True)\n",
    "net_name = 'resnet50'\n",
    "# 对layer3、4进行微调（fine_tuning），其它层冻结，保留参数，对fc层（输出层）进行完全训练 'layer1', 'layer2','layer3', 'layer4', 'fc'\n",
    "unfrozen_layers = ['layer3', 'layer4', 'fc'] \n",
    "# unfrozen_layers = ['fc'] \n",
    "\n",
    "# test6不使用droupout 0.2, 0.3, 0.6, 0.7, 0.8\n",
    "dropout_ratio = 0.3\n",
    "\n",
    "finetune_net = load_network(finetune_net, net_name, dropout_ratio, class_names, unfrozen_layers)\n",
    "\n",
    "print(f'training set: {len(training_set)}\\nvalidation set: {len(val_set)}\\ntest set: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果param_group = True，输出层中的模型参数将使用十倍的学习率\n",
    "def training_model(result_path, model_name, model, training_loader, val_loader, learning_rate, epochs, momentum, weight_decay, patience, n_epochs_stop, milestones):\n",
    "    \n",
    "    # \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    param_group = True\n",
    "    if param_group:\n",
    "        # 进行分组学习，调整不同分组参数的学习率\n",
    "        # fc层（输出层）的参数是随机初始化的，通常需要更高的学习率才能从头开始训练，这里设为学习率的10倍\n",
    "        params_1x = [ param for name, param in model.named_parameters()\n",
    "                      if name not in ['fc.0.weight', 'fc.0.bias', 'fc.3.weight', 'fc.3.bias'] ]\n",
    "                      # if name not in ['fc.weight', 'fc.bias'] ]\n",
    "        '''\n",
    "        optimizer = SGD([{'params': params_1x}, \n",
    "                         {'params': model.fc.parameters(), 'lr': learning_rate * 10}], \n",
    "                        lr = learning_rate, momentum = momentum, weight_decay = weight_decay)\n",
    "        '''\n",
    "        # test6用Adam算法进行梯度优化，进行微调的参数的学习率通常会比较小，而fc层需要大一些，设为其10倍\n",
    "        optimizer = Adam([{'params': params_1x},\n",
    "                          {'params': model.fc.parameters(), 'lr': learning_rate * 10}],\n",
    "                         lr = learning_rate, weight_decay = weight_decay)\n",
    "        \n",
    "    else:\n",
    "        '''\n",
    "        optimizer = SGD(model.parameters(), lr = learning_rate,\n",
    "                        weight_decay = weight_decay)\n",
    "        '''\n",
    "        optimizer = Adam(model.parameters(), lr = learning_rate,\n",
    "                         weight_decay = weight_decay)\n",
    "       \n",
    "    # test4 用Adam算法进行梯度优化\n",
    "    # optimizer = Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "    # 用随机梯度下降法进行梯度优化\n",
    "    # optimizer = SGD(model.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay)\n",
    "    # 用ReduceLROnPlateau优化减小学习率，基于验证指标的调整方法，当指标停止改善时，降低学习率\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience = patience, factor = 0.2, verbose = True)\n",
    "    '''\n",
    "    epoch <= m[1]         lr = 不变\n",
    "    m[1] < epoch <= m[2]  lr变为gamma倍\n",
    "    m[2] < epoch          lr变为gamma倍\n",
    "    '''\n",
    "    # scheduler = MultiStepLR(optimizer, milestones = milestones, gamma = 0.5, verbose = True)\n",
    "    \n",
    "    loaders     = {'training': training_loader, 'val': val_loader}\n",
    "    losses      = {'training': [], 'val': []}\n",
    "    accuracies  = {'training': [], 'val': []}\n",
    "    losses2     = {'training': [], 'val': []}\n",
    "    accuracies2 = {'training': [], 'val': []}\n",
    "    \n",
    "    y_test = []\n",
    "    preds  = []\n",
    "    \n",
    "    min_val_loss     = np.Inf\n",
    "    epochs_no_improv = 0\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            # \n",
    "            model = nn.DataParallel(model)\n",
    "            print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "        else :\n",
    "            print(f'Using {torch.cuda.device_count()} GPU')\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "    \n",
    "    start = time.time()\n",
    "    # 训练轮次（周期），包括训练和验证\n",
    "    for epoch in range(epochs):\n",
    "        for mode in ['training', 'val']:\n",
    "            if mode == 'training':\n",
    "                # \n",
    "                model.train()\n",
    "            if mode == 'val':\n",
    "                model.eval()\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            epoch_acc  = 0\n",
    "            samples    = 0\n",
    "            \n",
    "            # 训练批次，一个批量迭代一次\n",
    "            for i, (inputs, targets) in enumerate(loaders[mode]):\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs  = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                # 将梯度初始化为零，以批量为单位，一个batch的loss关于weight的导数是所有sample的loss关于weight的导数的累加和\n",
    "                optimizer.zero_grad()\n",
    "                # 前向传播求出预测的值\n",
    "                output = model(inputs)\n",
    "                # 求损失（一个批量的平均损失）\n",
    "                loss   = criterion(output, targets)\n",
    "                \n",
    "                if mode == 'training':\n",
    "                    # 反向传播求梯度\n",
    "                    loss.backward()\n",
    "                    # 更新所有模型参数，optimizer.step()常在每个mini-batch中，而scheduler.step()常在epoch里,不绝对，可以根据具体的需求来做\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    y_test.extend(targets.data.tolist())\n",
    "                    preds.extend(output.max(1)[1].tolist())\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    acc = accuracy_score(targets.data.cuda().cpu().numpy(), output.max(1)[1].cuda().cpu().numpy())\n",
    "                else:\n",
    "                    acc = accuracy_score(targets.data, output.max(1)[1])\n",
    "                # inputs.shape[0]为batch_size\n",
    "                # 训练集或验证集的损失总量、准确度总量、数据（sample）总量\n",
    "                epoch_loss += loss.data.item() * inputs.shape[0]\n",
    "                epoch_acc  += acc * inputs.shape[0]\n",
    "                samples    += inputs.shape[0]\n",
    "                losses2[mode].append(epoch_loss / samples)\n",
    "                accuracies2[mode].append(epoch_acc / samples)\n",
    "                \n",
    "                # 每隔 iterations // 5 次打印一次训练状态，每周期打印\n",
    "                if i % (len(loaders[mode]) // 5) == 0:\n",
    "                    print(f'[{mode}] Epoch {epoch + 1} / {epochs} Iteration {i + 1} / {len(loaders[mode])} Loss: {epoch_loss/samples:0.4f} Accuracy: {epoch_acc/samples:0.4f}  Already trained {time.time() - start :0.2f} s')\n",
    "                    # \n",
    "                    # losses2[mode].append(epoch_loss/samples)\n",
    "                    # accuracies2[mode].append(epoch_acc/samples)\n",
    "            # 整个训练集或验证集的平均损失、准确度   \n",
    "            epoch_loss /= samples\n",
    "            epoch_acc  /= samples\n",
    "            losses[mode].append(epoch_loss)\n",
    "            accuracies[mode].append(epoch_acc)\n",
    "            \n",
    "            print(f'[{mode}] Epoch {epoch + 1} / {epochs} Iteration {i + 1} / {len(loaders[mode])} Loss: {epoch_loss:0.4f} Accuracy: {epoch_acc:0.4f}  Already trained {time.time() - start :0.2f} s')\n",
    "            \n",
    "            \n",
    "            if mode == 'val':\n",
    "            # if (mode == 'val') & (epoch in milestones):\n",
    "                # 更新学习率lr, ReduceLROnPlateau\n",
    "                scheduler.step(epoch_loss)\n",
    "                # MultiStepLR\n",
    "                # scheduler.step()\n",
    "        \n",
    "        # Early stop，如果有 n_epochs_stop 轮训练没有提升，就提前结束训练\n",
    "        if mode == 'val':\n",
    "            if epoch_loss < min_val_loss:\n",
    "                torch.save(model.state_dict(), os.path.join(result_path, str(model_name) + '.pth'))\n",
    "                epochs_no_improv = 0\n",
    "                min_val_loss = epoch_loss\n",
    "            else:\n",
    "                epochs_no_improv += 1\n",
    "                print(f'Epochs with no improvement {epochs_no_improv}')\n",
    "                if epochs_no_improv == n_epochs_stop:\n",
    "                    print('Early stopping!')\n",
    "                    return model, (losses, accuracies), (losses2, accuracies2), y_test, preds\n",
    "                model.load_state_dict(torch.load(os.path.join(result_path, str(model_name) + '.pth')))\n",
    "                \n",
    "    # print(f'Training time: {time.time()-start} min.')\n",
    "    print(f'The whole process took {time.time() - start :0.2f} s.')\n",
    "    return model, (losses, accuracies), (losses2, accuracies2), y_test, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于模型参数是在ImageNet数据集上预训练的，并且足够好，因此通常只需要较小的学习率即可微调这些参数（layer1234: 0.0001, fc: 0.001）\n",
    "# 0.001+0.01, 0.0001+0.001, 0.00001+0.0001, 0.000001+0.00001\n",
    "learning_rate = 0.00001\n",
    "epochs        = 100\n",
    "# 0.9\n",
    "momentum      = 0.9\n",
    "# test6 使用权重衰减0.0005, 0.001\n",
    "weight_decay  = 0.0005\n",
    "# 可容忍的度量指标，没有提升的epoch数目，用于降低学习率\n",
    "patience      = 3\n",
    "\n",
    "milestones = [5, 9, 15, 25]\n",
    "\n",
    "# 早停法Early stop，如果有n轮训练没有提升，就提前结束训练\n",
    "n_epochs_stop = 6\n",
    "# results_path  = './'\n",
    "\n",
    "finetune_net, loss_acc, loss_acc2, y_test, preds = training_model(result_path, net_name, finetune_net, \n",
    "                                                       training_set_loader, val_set_loader, \n",
    "                                                       learning_rate, epochs, momentum, \n",
    "                                                       weight_decay, patience, n_epochs_stop, \n",
    "                                                       milestones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以周期为单位\n",
    "def plot_logs_classification(result_path, model_name, logs, epochs, max_loss, min_acc):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #if not os.path.exists(result_path+'/'+model_name):\n",
    "    #    os.makedirs(result_path+'/'+model_name)\n",
    "        \n",
    "    training_losses, training_accuracies, val_losses, val_accuracies = \\\n",
    "        logs[0]['training'], logs[1]['training'], logs[0]['val'], logs[1]['val']\n",
    "    \n",
    "    plt.figure(figsize = (36, 12))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(training_losses, color = 'royalblue', linestyle = 'solid')\n",
    "    plt.plot(val_losses, color = 'darkorange', linestyle = 'dashed')\n",
    "    plt.title('Loss', fontsize = 30)\n",
    "    plt.yticks(fontsize = 25)\n",
    "    plt.xticks(fontsize = 25)\n",
    "    #设置x轴的范围\n",
    "    plt.xlim(xmin = 0, xmax = epochs)\n",
    "    #设置y轴的范围\n",
    "    plt.ylim(ymin = 0, ymax = max_loss)\n",
    "    plt.legend(['Training Loss', 'Validation Loss'], fontsize = 25, loc = 'best')\n",
    "    plt.xlabel('Epoch', fontsize = 25)\n",
    "    plt.ylabel('Loss', fontsize = 25)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(training_accuracies, color = 'royalblue', linestyle = 'solid')\n",
    "    plt.plot(val_accuracies, color = 'darkorange', linestyle = 'dashed')\n",
    "    plt.title('Accuracy', fontsize = 30)\n",
    "    plt.yticks(fontsize = 25)\n",
    "    plt.xticks(fontsize = 25)\n",
    "    #设置x轴的范围\n",
    "    plt.xlim(xmin = 0, xmax = epochs)\n",
    "    #设置y轴的范围\n",
    "    plt.ylim(ymin = min_acc, ymax = 1)\n",
    "    plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize = 25, loc = 'lower right')\n",
    "    plt.xlabel('Epoch', fontsize = 25)\n",
    "    plt.ylabel('Accuracy', fontsize = 25)\n",
    "    plt.grid()\n",
    "    \n",
    "    # plt.savefig(result_path + str(model_name) + '_graph_v1.png', bbox_inches = 'tight')\n",
    "    plt.savefig(result_path + 'test6-2_RR-CJb4c4-RHF-v2_lr0.00001wd0.0005dr0.3.png', bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs   = 40\n",
    "max_loss = 5\n",
    "min_acc  = 0\n",
    "\n",
    "plot_logs_classification(result_path, net_name, loss_acc, epochs, max_loss, min_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以迭代次数为单位\n",
    "def plot_logs_classification2(result_path, model_name, logs, training_iter, val_iter, max_loss, min_acc):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #if not os.path.exists(result_path+'/'+model_name):\n",
    "    #    os.makedirs(result_path+'/'+model_name)\n",
    "        \n",
    "    training_losses, training_accuracies, val_losses, val_accuracies = \\\n",
    "        logs[0]['training'], logs[1]['training'], logs[0]['val'], logs[1]['val']\n",
    "    # 训练损失\n",
    "    plt.figure(figsize = (84, 64))\n",
    "    plt.subplot(221)\n",
    "    plt.plot(training_losses, color = 'royalblue', linestyle = 'solid')\n",
    "    # plt.plot(val_losses, color = 'darkorange', linestyle = 'dashed')\n",
    "    # plt.title('Training Loss', fontsize = 30)\n",
    "    plt.yticks(fontsize = 50)\n",
    "    plt.xticks(fontsize = 50)\n",
    "    #设置x轴的范围\n",
    "    plt.xlim( xmin = 0, xmax = training_iter)\n",
    "    #设置y轴的范围\n",
    "    plt.ylim(ymin = 0, ymax = max_loss)\n",
    "    # plt.legend(['Training Loss', 'Validation Loss'], fontsize = 25, loc = 'best')\n",
    "    plt.xlabel('Iteration', fontsize = 50)\n",
    "    plt.ylabel('Training Loss', fontsize = 50)\n",
    "    plt.grid()\n",
    "    \n",
    "    # 验证损失\n",
    "    plt.subplot(222)\n",
    "    # plt.plot(training_losses, color = 'royalblue', linestyle = 'solid')\n",
    "    plt.plot(val_losses, color = 'royalblue', linestyle = 'solid')\n",
    "    # plt.title('Loss', fontsize = 30)\n",
    "    plt.yticks(fontsize = 50)\n",
    "    plt.xticks(fontsize = 50)\n",
    "    #设置x轴的范围\n",
    "    plt.xlim( xmin = 0, xmax = val_iter)\n",
    "    #设置y轴的范围\n",
    "    plt.ylim(ymin = 0, ymax = max_loss)\n",
    "    # plt.legend(['Training Loss', 'Validation Loss'], fontsize = 25, loc = 'best')\n",
    "    plt.xlabel('Iteration', fontsize = 50)\n",
    "    plt.ylabel('Validation Loss', fontsize = 50)\n",
    "    plt.grid()\n",
    "    \n",
    "    # 训练精确度\n",
    "    plt.subplot(223)\n",
    "    plt.plot(training_accuracies, color = 'royalblue', linestyle = 'solid')\n",
    "    # plt.plot(val_accuracies, color = 'darkorange', linestyle = 'dashed')\n",
    "    # plt.title('Accuracy', fontsize = 30)\n",
    "    plt.yticks(fontsize = 50)\n",
    "    plt.xticks(fontsize = 50)\n",
    "    #设置x轴的范围\n",
    "    plt.xlim( xmin = 0, xmax = training_iter)\n",
    "    #设置y轴的范围\n",
    "    plt.ylim(ymin = min_acc, ymax = 1)\n",
    "    # plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize = 25, loc = 'best')\n",
    "    plt.xlabel('Iteration', fontsize = 50)\n",
    "    plt.ylabel('Training Accuracy', fontsize = 50)\n",
    "    plt.grid()\n",
    "    \n",
    "    # 验证精确度\n",
    "    plt.subplot(224)\n",
    "    # plt.plot(training_accuracies, color = 'royalblue', linestyle = 'solid')\n",
    "    plt.plot(val_accuracies, color = 'royalblue', linestyle = 'solid')\n",
    "    # plt.title('Accuracy', fontsize = 30)\n",
    "    plt.yticks(fontsize = 50)\n",
    "    plt.xticks(fontsize = 50)\n",
    "    #设置x轴的范围\n",
    "    plt.xlim( xmin = 0, xmax = val_iter)\n",
    "    #设置y轴的范围\n",
    "    plt.ylim(ymin = min_acc, ymax = 1)\n",
    "    # plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize = 25, loc = 'best')\n",
    "    plt.xlabel('Iteration', fontsize = 50)\n",
    "    plt.ylabel('Validation Accuracy', fontsize = 50)\n",
    "    plt.grid()\n",
    "    \n",
    "    # plt.subplots_adjust(wspace = 5, hspace = )\n",
    "    # plt.savefig(result_path + str(model_name) + '_graph_v1.png', bbox_inches = 'tight')\n",
    "    plt.savefig(result_path + 'test6-4_RR-CJb4c4-RHF-v2_lr0.00001wd0.0005dr0.3.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = epochs * 81\n",
    "val_iter      = epochs * 40\n",
    "max_loss      = 5\n",
    "min_acc       = 0\n",
    "\n",
    "plot_logs_classification2(result_path, net_name, loss_acc2, training_iter, val_iter, max_loss, min_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(result_path, model_name, model, test_loader):\n",
    "    model.load_state_dict(torch.load(result_path + str(model_name) + '.pth'))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            pred = model(inputs).data.cuda().cpu().numpy().copy()\n",
    "        else:\n",
    "            pred = model(inputs).data.numpy().copy()\n",
    "            \n",
    "        true = targets.numpy().copy()\n",
    "        preds.append(pred)\n",
    "        trues.append(true)\n",
    "\n",
    "        if i % (len(test_loader) // 5) == 0:\n",
    "            print(f'Iteration {i + 1} / {len(test_loader)}')\n",
    "    return np.concatenate(preds), np.concatenate(trues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test, y_true = test_model(result_path, net_name, finetune_net, test_set_loader)\n",
    "\n",
    "acc = accuracy_score(y_true, preds_test.argmax(1))\n",
    "score = f1_score(y_true, preds_test.argmax(1), average = 'micro')\n",
    "print(f'Accuracy: {acc: 0.4f}\\nMicro F1-score: {score: 0.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, preds_test.argmax(1), target_names = class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(result_path, model_name, y_true, preds, class_names, annot, figsize, fontsize) :\n",
    "    #if not os.path.exists(result_path+'/'+model_name):\n",
    "    #    os.makedirs(result_path+'/'+model_name)\n",
    "    \n",
    "    # F1-Score指标综合了Precision与Recall的产出的结果，取值范围从0到1的，1代表模型的输出最好，0代表模型的输出结果最差。\n",
    "    acc   = accuracy_score(y_true, preds.argmax(1))\n",
    "    score = f1_score(y_true, preds.argmax(1), average = 'micro')\n",
    "    cm    = confusion_matrix(y_true, preds.argmax(1))\n",
    "    # df_cm = pd.DataFrame(cm, index = class_names, columns = class_names)\n",
    "    class_index = [i for i in range(1, 121, 1)]\n",
    "    df_cm = pd.DataFrame(cm, index = class_index, columns = class_index)\n",
    "    np.set_printoptions(precision = 2)\n",
    "    \n",
    "    # string1   = 'Confusion Matrix for Test Data'\n",
    "    string1   = 'Confusion Matrix for Test Data'\n",
    "    # string2   = f'Accuracy is {acc:0.4f}; Micro F1-score is {score:0.4f}'\n",
    "    # title_str = string1.center(len(string2)) + '\\n' + string2\n",
    "    title_str = string1\n",
    "\n",
    "    plt.figure(figsize = figsize)\n",
    "    seaborn.set(font_scale = 1.2)\n",
    "    seaborn.heatmap(df_cm, annot = annot, annot_kws = {'size': 18, 'weight':'bold', 'color': 'white'}, fmt = 'd')\n",
    "    plt.yticks(fontsize = 18)\n",
    "    plt.xticks(fontsize = 18)\n",
    "    plt.ylabel('True Label', fontsize = fontsize)\n",
    "    plt.xlabel('Predicted Label', fontsize = fontsize)\n",
    "    plt.title(title_str, fontsize = fontsize)\n",
    "    \n",
    "    plt.savefig(os.path.join(result_path, 'test6_RR-CJb4c4-RHF-v2_lr0.00001wd0.0005dr0.3_conf_mat.png'), bbox_inches = 'tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混淆矩阵热力图可视化\n",
    "display_confusion_matrix(result_path, net_name, y_true, preds_test, class_names, annot = True, figsize = (40, 36), fontsize = 42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
